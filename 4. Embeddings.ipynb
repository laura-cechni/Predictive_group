{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ab10ef-dc39-407d-8027-0441dfc261d5",
   "metadata": {},
   "source": [
    "# 4. Embeddings\n",
    "## Making TF-IDF Movie Recommendations with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3966c0b0-e39a-4da6-abb0-a95b068689b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display, Code\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f5c6d4-3dcb-41e7-9631-10bc88d95f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'xryRHgoJc57nJONpRBRxNeGLQDNpE9a26jKazcLdkSQk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d671f07-926c-4aaf-8520-c8da8ed0541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_balance(api_key):\n",
    "    url = \"https://llm.api.ai8.io/credit\"\n",
    "    headers = {'Authorization': api_key}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        return response_data\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f8857b-01e9-4f75-aa01-f4c32fd306b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the LLM for TF-IDF and Word2Vec\n",
    "def get_resp_oai(input_text, model):\n",
    "    url = \"https://llm.api.ai8.io/query_llm\"\n",
    "    data = {\n",
    "        # Specify the model that you want to use\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You act as a highly intelligent recommendation system. Your job is to analyse the user and movie data and rank the top 20 candidate movies to the user is most likely to watch next based on their historical data provided and the last movie the user has watched.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "    }\n",
    "    headers = {'Authorization': api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        model_response = extract_message_oai(response_data)\n",
    "        return model_response\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}\n",
    "\n",
    "def extract_message_oai(response_data):\n",
    "    message_content = response_data.get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    # format the extracted message as markdown\n",
    "    markdown_content = \"---\\n\\n\" + message_content + \"\\n\\n---\"\n",
    "    return markdown_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f58faf1-8e86-48d1-8a33-3a6942a4e6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>director_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>genres_x</th>\n",
       "      <th>actor_1_name</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>actor_3_name</th>\n",
       "      <th>movie_imdb_link</th>\n",
       "      <th>language</th>\n",
       "      <th>...</th>\n",
       "      <th>Genre_Western</th>\n",
       "      <th>duration_category</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>budget_category</th>\n",
       "      <th>revenue_category</th>\n",
       "      <th>budget_revenue_ratio</th>\n",
       "      <th>log_budget</th>\n",
       "      <th>log_revenue</th>\n",
       "      <th>log_title_year</th>\n",
       "      <th>sqr_root_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>178.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>CCH Pounder</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>Wes Studi</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/?ref_=fn_t...</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>165-180 Minutes</td>\n",
       "      <td>7-8</td>\n",
       "      <td>Greater than 200 Million</td>\n",
       "      <td>Greater than 1.8 Billion</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>19.283571</td>\n",
       "      <td>21.748578</td>\n",
       "      <td>7.605890</td>\n",
       "      <td>13.341664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>169.0</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>Johnny Depp</td>\n",
       "      <td>Pirates of the Caribbean At Worlds End</td>\n",
       "      <td>Jack Davenport</td>\n",
       "      <td>http://www.imdb.com/title/tt0449088/?ref_=fn_t...</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>165-180 Minutes</td>\n",
       "      <td>7-8</td>\n",
       "      <td>Greater than 200 Million</td>\n",
       "      <td>800-1000 Million</td>\n",
       "      <td>0.312176</td>\n",
       "      <td>19.519293</td>\n",
       "      <td>20.683485</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>148.0</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>Christoph Waltz</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>Stephanie Sigman</td>\n",
       "      <td>http://www.imdb.com/title/tt2379713/?ref_=fn_t...</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>135-150 Minutes</td>\n",
       "      <td>6-7</td>\n",
       "      <td>Greater than 200 Million</td>\n",
       "      <td>800-1000 Million</td>\n",
       "      <td>0.278197</td>\n",
       "      <td>19.316769</td>\n",
       "      <td>20.596199</td>\n",
       "      <td>7.608871</td>\n",
       "      <td>12.165525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>164.0</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>Tom Hardy</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Joseph Gordon-Levitt</td>\n",
       "      <td>http://www.imdb.com/title/tt1345836/?ref_=fn_t...</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>150-165 Minutes</td>\n",
       "      <td>8-9</td>\n",
       "      <td>Greater than 200 Million</td>\n",
       "      <td>1-1.20 Billion</td>\n",
       "      <td>0.230429</td>\n",
       "      <td>19.336971</td>\n",
       "      <td>20.804790</td>\n",
       "      <td>7.607381</td>\n",
       "      <td>12.806248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Andrew Stanton</td>\n",
       "      <td>132.0</td>\n",
       "      <td>Samantha Morton</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>Daryl Sabara</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>Polly Walker</td>\n",
       "      <td>http://www.imdb.com/title/tt0401729/?ref_=fn_t...</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>120-135 Minutes</td>\n",
       "      <td>6-7</td>\n",
       "      <td>Greater than 200 Million</td>\n",
       "      <td>200-400 Million</td>\n",
       "      <td>0.915046</td>\n",
       "      <td>19.376192</td>\n",
       "      <td>19.464974</td>\n",
       "      <td>7.607381</td>\n",
       "      <td>11.489125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      director_name  duration      actor_2_name  \\\n",
       "0           0      James Cameron     178.0  Joel David Moore   \n",
       "1           1     Gore Verbinski     169.0     Orlando Bloom   \n",
       "2           2         Sam Mendes     148.0      Rory Kinnear   \n",
       "3           3  Christopher Nolan     164.0    Christian Bale   \n",
       "4           4     Andrew Stanton     132.0   Samantha Morton   \n",
       "\n",
       "                          genres_x     actor_1_name  \\\n",
       "0  Action|Adventure|Fantasy|Sci-Fi      CCH Pounder   \n",
       "1         Action|Adventure|Fantasy      Johnny Depp   \n",
       "2        Action|Adventure|Thriller  Christoph Waltz   \n",
       "3                  Action|Thriller        Tom Hardy   \n",
       "4          Action|Adventure|Sci-Fi     Daryl Sabara   \n",
       "\n",
       "                              movie_title          actor_3_name  \\\n",
       "0                                  Avatar             Wes Studi   \n",
       "1  Pirates of the Caribbean At Worlds End        Jack Davenport   \n",
       "2                                 Spectre      Stephanie Sigman   \n",
       "3                   The Dark Knight Rises  Joseph Gordon-Levitt   \n",
       "4                             John Carter          Polly Walker   \n",
       "\n",
       "                                     movie_imdb_link language  ...  \\\n",
       "0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...  English  ...   \n",
       "1  http://www.imdb.com/title/tt0449088/?ref_=fn_t...  English  ...   \n",
       "2  http://www.imdb.com/title/tt2379713/?ref_=fn_t...  English  ...   \n",
       "3  http://www.imdb.com/title/tt1345836/?ref_=fn_t...  English  ...   \n",
       "4  http://www.imdb.com/title/tt0401729/?ref_=fn_t...  English  ...   \n",
       "\n",
       "  Genre_Western  duration_category  rating_category           budget_category  \\\n",
       "0             0    165-180 Minutes              7-8  Greater than 200 Million   \n",
       "1             0    165-180 Minutes              7-8  Greater than 200 Million   \n",
       "2             0    135-150 Minutes              6-7  Greater than 200 Million   \n",
       "3             0    150-165 Minutes              8-9  Greater than 200 Million   \n",
       "4             0    120-135 Minutes              6-7  Greater than 200 Million   \n",
       "\n",
       "           revenue_category  budget_revenue_ratio log_budget  log_revenue  \\\n",
       "0  Greater than 1.8 Billion              0.085009  19.283571    21.748578   \n",
       "1          800-1000 Million              0.312176  19.519293    20.683485   \n",
       "2          800-1000 Million              0.278197  19.316769    20.596199   \n",
       "3            1-1.20 Billion              0.230429  19.336971    20.804790   \n",
       "4           200-400 Million              0.915046  19.376192    19.464974   \n",
       "\n",
       "   log_title_year  sqr_root_duration  \n",
       "0        7.605890          13.341664  \n",
       "1        7.604894          13.000000  \n",
       "2        7.608871          12.165525  \n",
       "3        7.607381          12.806248  \n",
       "4        7.607381          11.489125  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the final merged dataframe\n",
    "final_movies = pd.read_csv('final_merged_df.csv')\n",
    "final_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac229a1e-d8b0-4201-9daa-c3d6b51041bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'director_name', 'duration', 'actor_2_name', 'genres_x',\n",
       "       'actor_1_name', 'movie_title', 'actor_3_name', 'movie_imdb_link',\n",
       "       'language', 'country', 'title_year', 'imdb_score', 'budget',\n",
       "       'original_title', 'revenue', 'tagline', 'Genre_Action',\n",
       "       'Genre_Adventure', 'Genre_Animation', 'Genre_Biography', 'Genre_Comedy',\n",
       "       'Genre_Crime', 'Genre_Documentary', 'Genre_Drama', 'Genre_Family',\n",
       "       'Genre_Fantasy', 'Genre_Film-Noir', 'Genre_History', 'Genre_Horror',\n",
       "       'Genre_Music', 'Genre_Musical', 'Genre_Mystery', 'Genre_News',\n",
       "       'Genre_Romance', 'Genre_Sci-Fi', 'Genre_Sport', 'Genre_Thriller',\n",
       "       'Genre_War', 'Genre_Western', 'duration_category', 'rating_category',\n",
       "       'budget_category', 'revenue_category', 'budget_revenue_ratio',\n",
       "       'log_budget', 'log_revenue', 'log_title_year', 'sqr_root_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the structure of the dataframe for prompting\n",
    "final_movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc1075-2c76-47f3-80f6-464426d4ed7b",
   "metadata": {},
   "source": [
    "### Part I: Get TF-IDF and Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d1c9ef8-c227-462c-8d36-7a7bf1e330a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_prompt():\n",
    "    prompt = \"\"\"\n",
    "    **Task**: Create embeddings for a movie dataset using Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec (W2V) approaches. You must consider the appropriate data preprocessing steps. \n",
    "    \n",
    "    **Dataset Format**: The dataset is read in a CSV format with rows for each movie and columns including movie title, budget, imdb_score and other columns. These are all the name columns included in the dataset: ['director_name', 'duration', 'actor_2_name', 'genres_x', 'actor_1_name',\n",
    "       'movie_title', 'actor_3_name', 'movie_imdb_link', 'language', 'country',\n",
    "       'title_year', 'imdb_score', 'budget', 'original_title', 'revenue',\n",
    "       'tagline', 'Genre_Action', 'Genre_Adventure', 'Genre_Animation',\n",
    "       'Genre_Biography', 'Genre_Comedy', 'Genre_Crime', 'Genre_Documentary',\n",
    "       'Genre_Drama', 'Genre_Family', 'Genre_Fantasy', 'Genre_Film-Noir',\n",
    "       'Genre_History', 'Genre_Horror', 'Genre_Music', 'Genre_Musical',\n",
    "       'Genre_Mystery', 'Genre_News', 'Genre_Romance', 'Genre_Sci-Fi',\n",
    "       'Genre_Sport', 'Genre_Thriller', 'Genre_War', 'Genre_Western',\n",
    "       'duration_category', 'rating_category', 'budget_category',\n",
    "       'revenue_category', 'budget_revenue_ratio', 'log_budget', 'log_revenue',\n",
    "       'log_title_year', 'sqr_root_duration'].\n",
    "\n",
    "        This is the head of the dataset in json format, for your reference: Dataset Head = [{\"index\":0,\"director_name\":\"James Cameron\",\"duration\":\"178.0\",\"actor_2_name\":\"Joel David Moore\",\"genres_x\":\"Action|Adventure|Fantasy|Sci-Fi\",\"actor_1_name\":\"CCH Pounder\",\"movie_title\":\"Avatar\",\"actor_3_name\":\"Wes Studi\",\"movie_imdb_link\":\"http://www.imdb.com/title/tt0499549/?ref_=fn_tt_tt_1\",\"language\":\"English\",\"country\":\"USA\",\"title_year\":\"2009.0\",\"imdb_score\":7.9,\"budget\":237000000,\"original_title\":\"Avatar\",\"revenue\":\"2787965087.0\",\"tagline\":\"Enter the World of Pandora.\",\"Genre_Action\":1,\"Genre_Adventure\":1,\"Genre_Animation\":0,\"Genre_Biography\":0},{\"index\":1,\"director_name\":\"Gore Verbinski\",\"duration\":\"169.0\",\"actor_2_name\":\"Orlando Bloom\",\"genres_x\":\"Action|Adventure|Fantasy\",\"actor_1_name\":\"Johnny Depp\",\"movie_title\":\"Pirates of the Caribbean At Worlds End\",\"actor_3_name\":\"Jack Davenport\",\"movie_imdb_link\":\"http://www.imdb.com/title/tt0449088/?ref_=fn_tt_tt_1\",\"language\":\"English\",\"country\":\"USA\",\"title_year\":\"2007.0\",\"imdb_score\":7.1,\"budget\":300000000,\"original_title\":\"Pirates of the Caribbean At Worlds End\",\"revenue\":\"961000000.0\",\"tagline\":\"At the end of the world, the adventure begins.\",\"Genre_Action\":1,\"Genre_Adventure\":1,\"Genre_Animation\":0,\"Genre_Biography\":0},{\"index\":2,\"director_name\":\"Sam Mendes\",\"duration\":\"148.0\",\"actor_2_name\":\"Rory Kinnear\",\"genres_x\":\"Action|Adventure|Thriller\",\"actor_1_name\":\"Christoph Waltz\",\"movie_title\":\"Spectre\",\"actor_3_name\":\"Stephanie Sigman\",\"movie_imdb_link\":\"http://www.imdb.com/title/tt2379713/?ref_=fn_tt_tt_1\",\"language\":\"English\",\"country\":\"UK\",\"title_year\":\"2015.0\",\"imdb_score\":6.8,\"budget\":245000000,\"original_title\":\"Spectre\",\"revenue\":\"880674609.0\",\"tagline\":\"A Plan No One Escapes\",\"Genre_Action\":1,\"Genre_Adventure\":1,\"Genre_Animation\":0,\"Genre_Biography\":0},{\"index\":3,\"director_name\":\"Christopher Nolan\",\"duration\":\"164.0\",\"actor_2_name\":\"Christian Bale\",\"genres_x\":\"Action|Thriller\",\"actor_1_name\":\"Tom Hardy\",\"movie_title\":\"The Dark Knight Rises\",\"actor_3_name\":\"Joseph Gordon-Levitt\",\"movie_imdb_link\":\"http://www.imdb.com/title/tt1345836/?ref_=fn_tt_tt_1\",\"language\":\"English\",\"country\":\"USA\",\"title_year\":\"2012.0\",\"imdb_score\":8.5,\"budget\":250000000,\"original_title\":\"The Dark Knight Rises\",\"revenue\":\"1084939099.0\",\"tagline\":\"The Legend Ends\",\"Genre_Action\":1,\"Genre_Adventure\":0,\"Genre_Animation\":0,\"Genre_Biography\":0},{\"index\":4,\"director_name\":\"Andrew Stanton\",\"duration\":\"132.0\",\"actor_2_name\":\"Samantha Morton\",\"genres_x\":\"Action|Adventure|Sci-Fi\",\"actor_1_name\":\"Daryl Sabara\",\"movie_title\":\"John Carter\",\"actor_3_name\":\"Polly Walker\",\"movie_imdb_link\":\"http://www.imdb.com/title/tt0401729/?ref_=fn_tt_tt_1\",\"language\":\"English\",\"country\":\"USA\",\"title_year\":\"2012.0\",\"imdb_score\":6.6,\"budget\":260000000,\"original_title\":\"John Carter\",\"revenue\":\"284139100.0\",\"tagline\":\"Lost in our world, found in another.\",\"Genre_Action\":1,\"Genre_Adventure\":1,\"Genre_Animation\":0,\"Genre_Biography\":0}]\n",
    "        \n",
    "        More information about the dataset (we get this when executing movies_dataset_name.info()): <class 'pandas.core.frame.DataFrame'>\n",
    "        Index: 4144 entries, 0 to 4161\n",
    "        Data columns (total 48 columns):\n",
    "         #   Column                Non-Null Count  Dtype   \n",
    "        ---  ------                --------------  -----   \n",
    "         0   director_name         4144 non-null   object  \n",
    "         1   duration              4142 non-null   float64 \n",
    "         2   actor_2_name          4140 non-null   object  \n",
    "         3   genres_x              4144 non-null   object  \n",
    "         4   actor_1_name          4141 non-null   object  \n",
    "         5   movie_title           4144 non-null   object  \n",
    "         6   actor_3_name          4135 non-null   object  \n",
    "         7   movie_imdb_link       4144 non-null   object  \n",
    "         8   language              4137 non-null   object  \n",
    "         9   country               4144 non-null   object  \n",
    "         10  title_year            4144 non-null   float64 \n",
    "         11  imdb_score            4144 non-null   float64\n",
    "         12  budget                4144 non-null   int64   \n",
    "         13  original_title        4144 non-null   object  \n",
    "         14  revenue               4144 non-null   float64 \n",
    "         15  tagline               3584 non-null   object  \n",
    "         16  Genre_Action          4144 non-null   int64   \n",
    "         17  Genre_Adventure       4144 non-null   int64   \n",
    "         18  Genre_Animation       4144 non-null   int64   \n",
    "         19  Genre_Biography       4144 non-null   int64   \n",
    "         20  Genre_Comedy          4144 non-null   int64   \n",
    "         21  Genre_Crime           4144 non-null   int64   \n",
    "         22  Genre_Documentary     4144 non-null   int64   \n",
    "         23  Genre_Drama           4144 non-null   int64   \n",
    "         24  Genre_Family          4144 non-null   int64   \n",
    "         25  Genre_Fantasy         4144 non-null   int64   \n",
    "         26  Genre_Film-Noir       4144 non-null   int64   \n",
    "         27  Genre_History         4144 non-null   int64   \n",
    "         28  Genre_Horror          4144 non-null   int64   \n",
    "         29  Genre_Music           4144 non-null   int64   \n",
    "         30  Genre_Musical         4144 non-null   int64   \n",
    "         31  Genre_Mystery         4144 non-null   int64   \n",
    "         32  Genre_News            4144 non-null   int64   \n",
    "         33  Genre_Romance         4144 non-null   int64   \n",
    "         34  Genre_Sci-Fi          4144 non-null   int64   \n",
    "         35  Genre_Sport           4144 non-null   int64   \n",
    "         36  Genre_Thriller        4144 non-null   int64   \n",
    "         37  Genre_War             4144 non-null   int64   \n",
    "         38  Genre_Western         4144 non-null   int64   \n",
    "         39  duration_category     4142 non-null   category\n",
    "         40  rating_category       4144 non-null   category\n",
    "         41  budget_category       3430 non-null   category\n",
    "         42  revenue_category      3126 non-null   category\n",
    "        43  budget_revenue_ratio  3552 non-null   float64 \n",
    "         44  log_budget            4144 non-null   float64 \n",
    "         45  log_revenue           4144 non-null   float64 \n",
    "         46  log_title_year        4144 non-null   float64 \n",
    "         47  sqr_root_duration     4142 non-null   float64 \n",
    "        dtypes: category(4), float64(9), int64(24), object(11)\n",
    "        memory usage: 1.4+ MB\n",
    "\n",
    "    **Dataset Description:** You have been provided with a movie dataset containing 48 features of 4144 movies. Each entry in the dataset represents a single movie along with its associated features.\n",
    "\n",
    "    **Objective:**\n",
    "        1. TF-IDF Embeddings: Implement TF-IDF vectorization on the movie dataset to generate embeddings.\n",
    "        2. Word2Vec Embeddings: Implement Word2Vec model training on the movie dataset to generate embeddings.\n",
    "    \n",
    "    **Deliverables:**\n",
    "        1. Python code implementing the generation of TF-IDF and Word2Vec embeddings for the provided movie dataset.\n",
    "        2. Embeddings files or data structures containing the embeddings generated for each movie.\n",
    "   \n",
    "    **Requirements:**\n",
    "        * Use Python for implementation.\n",
    "        * Generate executable jupyter notebook code chunks.\n",
    "        * Include the appropriate data preprocessing steps. \n",
    "        * Utilise appropriate libraries (e.g., scikit-learn, Gensim) for TF-IDF and Word2Vec implementations.\n",
    "        * Provide comments or documentation within the code to explain the process and any parameters used.\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e73565c-2650-47ce-9713-703992b2b309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color: #34568B;'>\n",
       "\n",
       "---\n",
       "\n",
       "To achieve the objective of creating embeddings for the given movie dataset using Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec (W2V) approaches, I'll walk you through both methods step by step, including the appropriate data preprocessing techniques.\n",
       "\n",
       "### 1. Data Preprocessing\n",
       "\n",
       "Data preprocessing is crucial for both TF-IDF and Word2Vec models to ensure the models have clean and relevant data to work with. Given our dataset, we'll focus on text-based features such as `movie_title`, `original_title`, `tagline`, and `genres_x` as they are likely to provide the most meaningful context for generating embeddings.\n",
       "\n",
       "First, let's prepare our environment and load the dataset:\n",
       "\n",
       "```python\n",
       "# Import necessary libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "from gensim.models import Word2Vec\n",
       "import string\n",
       "import re\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.tokenize import word_tokenize\n",
       "from sklearn.preprocessing import normalize\n",
       "import nltk\n",
       "\n",
       "nltk.download('punkt')\n",
       "nltk.download('stopwords')\n",
       "\n",
       "# Load the dataset (assuming the file path is correct)\n",
       "dataset_path = 'path_to_your_dataset.csv' # Please replace this with your dataset path\n",
       "movies_dataset = pd.read_csv(dataset_path)\n",
       "\n",
       "# Preview the dataset\n",
       "print(movies_dataset.head())\n",
       "```\n",
       "\n",
       "#### Cleaning and Preparing Text Data\n",
       "\n",
       "For both TF-IDF and Word2Vec models, our text data must be cleaned and preprocessed. This involves converting to lowercase, removing punctuation, stop words, and tokenization.\n",
       "\n",
       "```python\n",
       "# Function to clean text data\n",
       "def clean_text(text):\n",
       "    if not pd.isnull(text):\n",
       "        # Convert text to lowercase\n",
       "        text = text.lower()\n",
       "        # Remove numbers and punctuation\n",
       "        text = re.sub(r'[^\\w\\s]', '', text)\n",
       "        # Tokenize the text\n",
       "        tokens = word_tokenize(text)\n",
       "        # Remove stop words\n",
       "        stop_words = set(stopwords.words('english'))\n",
       "        tokens = [w for w in tokens if not w in stop_words]\n",
       "        # Join the tokens back into a string\n",
       "        text = ' '.join(tokens)\n",
       "    else:\n",
       "        text = ''\n",
       "    return text\n",
       "\n",
       "# Columns to preprocess\n",
       "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
       "\n",
       "# Apply cleaning function to text columns\n",
       "for column in text_columns:\n",
       "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
       "\n",
       "# Combine relevant text features into a single text corpus per movie\n",
       "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
       "```\n",
       "\n",
       "### 2. TF-IDF Embeddings\n",
       "\n",
       "Now, let's create TF-IDF embeddings for our dataset.\n",
       "\n",
       "```python\n",
       "# Initialize TF-IDF Vectorizer\n",
       "tfidf_vectorizer = TfidfVectorizer()\n",
       "\n",
       "# Generate TF-IDF embeddings\n",
       "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\n",
       "\n",
       "# Normalize TF-IDF embeddings\n",
       "tfidf_matrix = normalize(tfidf_matrix)\n",
       "```\n",
       "\n",
       "### 3. Word2Vec Embeddings\n",
       "\n",
       "For Word2Vec, we'll need to prepare our corpus slightly differently.\n",
       "\n",
       "```python\n",
       "# Prepare corpus for Word2Vec\n",
       "corpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\n",
       "\n",
       "# Initialize and train the Word2Vec model\n",
       "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "# Generate Word2Vec embeddings (averaged across all words in the document's corpus)\n",
       "word2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\n",
       "```\n",
       "\n",
       "### Deliverables\n",
       "\n",
       "1. **TF-IDF Embeddings:** The `tfidf_matrix` from the TF-IDF section contains the embeddings for each movie based on the `text_corpus`. You can save or use this matrix as needed.\n",
       "\n",
       "2. **Word2Vec Embeddings:** The `word2vec_embeddings` array from the Word2Vec section contains the embeddings for each movie. Each embedding is the averaged vector of all word vectors in the movie's `text_corpus`.\n",
       "\n",
       "### Additional Notes:\n",
       "\n",
       "- The implemented preprocessing steps, such as tokenization and stop words removal, are essential for both models to focus on the most meaningful words in the texts.\n",
       "- Parameters for both TF-IDF and Word2Vec (such as `vector_size`, `window`, etc.) can be tuned according to specific requirements or based on experimentation to improve the performance of downstream tasks.\n",
       "- For more accurate or contextual embeddings, consider additional preprocessing or feature engineering steps based on the dataset's characteristics and the target application.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = define_prompt()\n",
    "msg_1 = get_resp_oai(prompt,\"gpt-4-0125-preview\")\n",
    "# msg_1\n",
    "display(Markdown(\"<div style='color: #34568B;'>\\n\\n\" + msg_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec0ff976-735c-45cb-893e-3d1ff1b43f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_extraction_prompt(resp_str):\n",
    "    return f\"Can you transform this string: {resp_str} into an executable string by extracting the python code and keeping the rest of the information as comments? Please place comment symbols where necessary and keep in mind that the given response string will be executed in a python code chunk using the function:'exec(given_response_string).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bae2a000-1e21-4cae-83ff-5fdd1039861b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"o\">---</span>\n",
       "\n",
       "<span class=\"err\">```</span><span class=\"n\">python</span>\n",
       "<span class=\"c1\"># To achieve the objective of creating embeddings for the given movie dataset using Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec (W2V) approaches, I&#39;ll walk you through both methods step by step, including the appropriate data preprocessing techniques.</span>\n",
       "\n",
       "<span class=\"c1\"># 1. Data Preprocessing</span>\n",
       "\n",
       "<span class=\"c1\"># Data preprocessing is crucial for both TF-IDF and Word2Vec models to ensure the models have clean and relevant data to work with. Given our dataset, we&#39;ll focus on text-based features such as `movie_title`, `original_title`, `tagline`, and `genres_x` as they are likely to provide the most meaningful context for generating embeddings.</span>\n",
       "\n",
       "<span class=\"c1\"># First, let&#39;s prepare our environment and load the dataset:</span>\n",
       "\n",
       "<span class=\"c1\"># Import necessary libraries</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">TfidfVectorizer</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">gensim.models</span> <span class=\"kn\">import</span> <span class=\"n\">Word2Vec</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">string</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">stopwords</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.tokenize</span> <span class=\"kn\">import</span> <span class=\"n\">word_tokenize</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">normalize</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n",
       "\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;punkt&#39;</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;stopwords&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Load the dataset (assuming the file path is correct)</span>\n",
       "<span class=\"n\">dataset_path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;path_to_your_dataset.csv&#39;</span> <span class=\"c1\"># Please replace this with your dataset path</span>\n",
       "<span class=\"n\">movies_dataset</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">dataset_path</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Preview the dataset</span>\n",
       "<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n",
       "\n",
       "<span class=\"c1\"># Cleaning and Preparing Text Data</span>\n",
       "\n",
       "<span class=\"c1\"># For both TF-IDF and Word2Vec models, our text data must be cleaned and preprocessed. This involves converting to lowercase, removing punctuation, stop words, and tokenization.</span>\n",
       "\n",
       "<span class=\"c1\"># Function to clean text data</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">clean_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">isnull</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># Convert text to lowercase</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span>\n",
       "        <span class=\"c1\"># Remove numbers and punctuation</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;[^\\w\\s]&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;&#39;</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\"># Tokenize the text</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\"># Remove stop words</span>\n",
       "        <span class=\"n\">stop_words</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">stopwords</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s1\">&#39;english&#39;</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">stop_words</span><span class=\"p\">]</span>\n",
       "        <span class=\"c1\"># Join the tokens back into a string</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">text</span>\n",
       "\n",
       "<span class=\"c1\"># Columns to preprocess</span>\n",
       "<span class=\"n\">text_columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"c1\"># Apply cleaning function to text columns</span>\n",
       "<span class=\"k\">for</span> <span class=\"n\">column</span> <span class=\"ow\">in</span> <span class=\"n\">text_columns</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">clean_text</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Combine relevant text features into a single text corpus per movie</span>\n",
       "<span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"c1\"># 2. TF-IDF Embeddings</span>\n",
       "\n",
       "<span class=\"c1\"># Now, let&#39;s create TF-IDF embeddings for our dataset.</span>\n",
       "\n",
       "<span class=\"c1\"># Initialize TF-IDF Vectorizer</span>\n",
       "<span class=\"n\">tfidf_vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">TfidfVectorizer</span><span class=\"p\">()</span>\n",
       "\n",
       "<span class=\"c1\"># Generate TF-IDF embeddings</span>\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tfidf_vectorizer</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">])</span>\n",
       "\n",
       "<span class=\"c1\"># Normalize TF-IDF embeddings</span>\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">tfidf_matrix</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># 3. Word2Vec Embeddings</span>\n",
       "\n",
       "<span class=\"c1\"># For Word2Vec, we&#39;ll need to prepare our corpus slightly differently.</span>\n",
       "\n",
       "<span class=\"c1\"># Prepare corpus for Word2Vec</span>\n",
       "<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]]</span>\n",
       "\n",
       "<span class=\"c1\"># Initialize and train the Word2Vec model</span>\n",
       "<span class=\"n\">word2vec_model</span> <span class=\"o\">=</span> <span class=\"n\">Word2Vec</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"o\">=</span><span class=\"n\">corpus</span><span class=\"p\">,</span> <span class=\"n\">vector_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">min_count</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Generate Word2Vec embeddings (averaged across all words in the document&#39;s corpus)</span>\n",
       "<span class=\"n\">word2vec_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">([</span><span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">document</span> <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">document</span> <span class=\"ow\">in</span> <span class=\"n\">corpus</span><span class=\"p\">])</span>\n",
       "\n",
       "<span class=\"c1\"># Deliverables</span>\n",
       "\n",
       "<span class=\"c1\"># 1. **TF-IDF Embeddings:** The `tfidf_matrix` from the TF-IDF section contains the embeddings for each movie based on the `text_corpus`. You can save or use this matrix as needed.</span>\n",
       "\n",
       "<span class=\"c1\"># 2. **Word2Vec Embeddings:** The `word2vec_embeddings` array from the Word2Vec section contains the embeddings for each movie. Each embedding is the averaged vector of all word vectors in the movie&#39;s `text_corpus`.</span>\n",
       "\n",
       "<span class=\"c1\"># Additional Notes:</span>\n",
       "\n",
       "<span class=\"c1\"># - The implemented preprocessing steps, such as tokenization and stop words removal, are essential for both models to focus on the most meaningful words in the texts.</span>\n",
       "<span class=\"c1\"># - Parameters for both TF-IDF and Word2Vec (such as `vector_size`, `window`, etc.) can be tuned according to specific requirements or based on experimentation to improve the performance of downstream tasks.</span>\n",
       "<span class=\"c1\"># - For more accurate or contextual embeddings, consider additional preprocessing or feature engineering steps based on the dataset&#39;s characteristics and the target application.</span>\n",
       "<span class=\"err\">```</span>\n",
       "\n",
       "<span class=\"o\">---</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\\PY{n}{python}\n",
       "\\PY{c+c1}{\\PYZsh{} To achieve the objective of creating embeddings for the given movie dataset using Term Frequency\\PYZhy{}Inverse Document Frequency (TF\\PYZhy{}IDF) and Word2Vec (W2V) approaches, I\\PYZsq{}ll walk you through both methods step by step, including the appropriate data preprocessing techniques.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} 1. Data Preprocessing}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Data preprocessing is crucial for both TF\\PYZhy{}IDF and Word2Vec models to ensure the models have clean and relevant data to work with. Given our dataset, we\\PYZsq{}ll focus on text\\PYZhy{}based features such as `movie\\PYZus{}title`, `original\\PYZus{}title`, `tagline`, and `genres\\PYZus{}x` as they are likely to provide the most meaningful context for generating embeddings.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} First, let\\PYZsq{}s prepare our environment and load the dataset:}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Import necessary libraries}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{pandas} \\PY{k}{as} \\PY{n+nn}{pd}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{feature\\PYZus{}extraction}\\PY{n+nn}{.}\\PY{n+nn}{text} \\PY{k+kn}{import} \\PY{n}{TfidfVectorizer}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{gensim}\\PY{n+nn}{.}\\PY{n+nn}{models} \\PY{k+kn}{import} \\PY{n}{Word2Vec}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{string}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{re}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{corpus} \\PY{k+kn}{import} \\PY{n}{stopwords}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{tokenize} \\PY{k+kn}{import} \\PY{n}{word\\PYZus{}tokenize}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{preprocessing} \\PY{k+kn}{import} \\PY{n}{normalize}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{nltk}\n",
       "\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{punkt}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{stopwords}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Load the dataset (assuming the file path is correct)}\n",
       "\\PY{n}{dataset\\PYZus{}path} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{path\\PYZus{}to\\PYZus{}your\\PYZus{}dataset.csv}\\PY{l+s+s1}{\\PYZsq{}} \\PY{c+c1}{\\PYZsh{} Please replace this with your dataset path}\n",
       "\\PY{n}{movies\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{read\\PYZus{}csv}\\PY{p}{(}\\PY{n}{dataset\\PYZus{}path}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Preview the dataset}\n",
       "\\PY{n+nb}{print}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{o}{.}\\PY{n}{head}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Cleaning and Preparing Text Data}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} For both TF\\PYZhy{}IDF and Word2Vec models, our text data must be cleaned and preprocessed. This involves converting to lowercase, removing punctuation, stop words, and tokenization.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Function to clean text data}\n",
       "\\PY{k}{def} \\PY{n+nf}{clean\\PYZus{}text}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{isnull}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} Convert text to lowercase}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{text}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{} Remove numbers and punctuation}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{re}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+s+sa}{r}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{[\\PYZca{}}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{w}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{s]}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{} Tokenize the text}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{} Remove stop words}\n",
       "        \\PY{n}{stop\\PYZus{}words} \\PY{o}{=} \\PY{n+nb}{set}\\PY{p}{(}\\PY{n}{stopwords}\\PY{o}{.}\\PY{n}{words}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{english}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{p}{[}\\PY{n}{w} \\PY{k}{for} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{tokens} \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{stop\\PYZus{}words}\\PY{p}{]}\n",
       "        \\PY{c+c1}{\\PYZsh{} Join the tokens back into a string}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{tokens}\\PY{p}{)}\n",
       "    \\PY{k}{else}\\PY{p}{:}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "    \\PY{k}{return} \\PY{n}{text}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Columns to preprocess}\n",
       "\\PY{n}{text\\PYZus{}columns} \\PY{o}{=} \\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Apply cleaning function to text columns}\n",
       "\\PY{k}{for} \\PY{n}{column} \\PY{o+ow}{in} \\PY{n}{text\\PYZus{}columns}\\PY{p}{:}\n",
       "    \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]}\\PY{o}{.}\\PY{n}{apply}\\PY{p}{(}\\PY{n}{clean\\PYZus{}text}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Combine relevant text features into a single text corpus per movie}\n",
       "\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} 2. TF\\PYZhy{}IDF Embeddings}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Now, let\\PYZsq{}s create TF\\PYZhy{}IDF embeddings for our dataset.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Initialize TF\\PYZhy{}IDF Vectorizer}\n",
       "\\PY{n}{tfidf\\PYZus{}vectorizer} \\PY{o}{=} \\PY{n}{TfidfVectorizer}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Generate TF\\PYZhy{}IDF embeddings}\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{tfidf\\PYZus{}vectorizer}\\PY{o}{.}\\PY{n}{fit\\PYZus{}transform}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Normalize TF\\PYZhy{}IDF embeddings}\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{normalize}\\PY{p}{(}\\PY{n}{tfidf\\PYZus{}matrix}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} 3. Word2Vec Embeddings}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} For Word2Vec, we\\PYZsq{}ll need to prepare our corpus slightly differently.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Prepare corpus for Word2Vec}\n",
       "\\PY{n}{corpus} \\PY{o}{=} \\PY{p}{[}\\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{doc}\\PY{p}{)} \\PY{k}{for} \\PY{n}{doc} \\PY{o+ow}{in} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Initialize and train the Word2Vec model}\n",
       "\\PY{n}{word2vec\\PYZus{}model} \\PY{o}{=} \\PY{n}{Word2Vec}\\PY{p}{(}\\PY{n}{sentences}\\PY{o}{=}\\PY{n}{corpus}\\PY{p}{,} \\PY{n}{vector\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{100}\\PY{p}{,} \\PY{n}{window}\\PY{o}{=}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{n}{min\\PYZus{}count}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Generate Word2Vec embeddings (averaged across all words in the document\\PYZsq{}s corpus)}\n",
       "\\PY{n}{word2vec\\PYZus{}embeddings} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{array}\\PY{p}{(}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{p}{[}\\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{[}\\PY{n}{word}\\PY{p}{]} \\PY{k}{for} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{document} \\PY{k}{if} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{]} \\PY{o+ow}{or} \\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{zeros}\\PY{p}{(}\\PY{l+m+mi}{100}\\PY{p}{)}\\PY{p}{]}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{0}\\PY{p}{)} \\PY{k}{for} \\PY{n}{document} \\PY{o+ow}{in} \\PY{n}{corpus}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Deliverables}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} 1. **TF\\PYZhy{}IDF Embeddings:** The `tfidf\\PYZus{}matrix` from the TF\\PYZhy{}IDF section contains the embeddings for each movie based on the `text\\PYZus{}corpus`. You can save or use this matrix as needed.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} 2. **Word2Vec Embeddings:** The `word2vec\\PYZus{}embeddings` array from the Word2Vec section contains the embeddings for each movie. Each embedding is the averaged vector of all word vectors in the movie\\PYZsq{}s `text\\PYZus{}corpus`.}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Additional Notes:}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} \\PYZhy{} The implemented preprocessing steps, such as tokenization and stop words removal, are essential for both models to focus on the most meaningful words in the texts.}\n",
       "\\PY{c+c1}{\\PYZsh{} \\PYZhy{} Parameters for both TF\\PYZhy{}IDF and Word2Vec (such as `vector\\PYZus{}size`, `window`, etc.) can be tuned according to specific requirements or based on experimentation to improve the performance of downstream tasks.}\n",
       "\\PY{c+c1}{\\PYZsh{} \\PYZhy{} For more accurate or contextual embeddings, consider additional preprocessing or feature engineering steps based on the dataset\\PYZsq{}s characteristics and the target application.}\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\n",
       "\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "```python\n",
       "# To achieve the objective of creating embeddings for the given movie dataset using Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec (W2V) approaches, I'll walk you through both methods step by step, including the appropriate data preprocessing techniques.\n",
       "\n",
       "# 1. Data Preprocessing\n",
       "\n",
       "# Data preprocessing is crucial for both TF-IDF and Word2Vec models to ensure the models have clean and relevant data to work with. Given our dataset, we'll focus on text-based features such as `movie_title`, `original_title`, `tagline`, and `genres_x` as they are likely to provide the most meaningful context for generating embeddings.\n",
       "\n",
       "# First, let's prepare our environment and load the dataset:\n",
       "\n",
       "# Import necessary libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "from gensim.models import Word2Vec\n",
       "import string\n",
       "import re\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.tokenize import word_tokenize\n",
       "from sklearn.preprocessing import normalize\n",
       "import nltk\n",
       "\n",
       "nltk.download('punkt')\n",
       "nltk.download('stopwords')\n",
       "\n",
       "# Load the dataset (assuming the file path is correct)\n",
       "dataset_path = 'path_to_your_dataset.csv' # Please replace this with your dataset path\n",
       "movies_dataset = pd.read_csv(dataset_path)\n",
       "\n",
       "# Preview the dataset\n",
       "print(movies_dataset.head())\n",
       "\n",
       "# Cleaning and Preparing Text Data\n",
       "\n",
       "# For both TF-IDF and Word2Vec models, our text data must be cleaned and preprocessed. This involves converting to lowercase, removing punctuation, stop words, and tokenization.\n",
       "\n",
       "# Function to clean text data\n",
       "def clean_text(text):\n",
       "    if not pd.isnull(text):\n",
       "        # Convert text to lowercase\n",
       "        text = text.lower()\n",
       "        # Remove numbers and punctuation\n",
       "        text = re.sub(r'[^\\w\\s]', '', text)\n",
       "        # Tokenize the text\n",
       "        tokens = word_tokenize(text)\n",
       "        # Remove stop words\n",
       "        stop_words = set(stopwords.words('english'))\n",
       "        tokens = [w for w in tokens if not w in stop_words]\n",
       "        # Join the tokens back into a string\n",
       "        text = ' '.join(tokens)\n",
       "    else:\n",
       "        text = ''\n",
       "    return text\n",
       "\n",
       "# Columns to preprocess\n",
       "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
       "\n",
       "# Apply cleaning function to text columns\n",
       "for column in text_columns:\n",
       "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
       "\n",
       "# Combine relevant text features into a single text corpus per movie\n",
       "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
       "\n",
       "# 2. TF-IDF Embeddings\n",
       "\n",
       "# Now, let's create TF-IDF embeddings for our dataset.\n",
       "\n",
       "# Initialize TF-IDF Vectorizer\n",
       "tfidf_vectorizer = TfidfVectorizer()\n",
       "\n",
       "# Generate TF-IDF embeddings\n",
       "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\n",
       "\n",
       "# Normalize TF-IDF embeddings\n",
       "tfidf_matrix = normalize(tfidf_matrix)\n",
       "\n",
       "# 3. Word2Vec Embeddings\n",
       "\n",
       "# For Word2Vec, we'll need to prepare our corpus slightly differently.\n",
       "\n",
       "# Prepare corpus for Word2Vec\n",
       "corpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\n",
       "\n",
       "# Initialize and train the Word2Vec model\n",
       "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "# Generate Word2Vec embeddings (averaged across all words in the document's corpus)\n",
       "word2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\n",
       "\n",
       "# Deliverables\n",
       "\n",
       "# 1. **TF-IDF Embeddings:** The `tfidf_matrix` from the TF-IDF section contains the embeddings for each movie based on the `text_corpus`. You can save or use this matrix as needed.\n",
       "\n",
       "# 2. **Word2Vec Embeddings:** The `word2vec_embeddings` array from the Word2Vec section contains the embeddings for each movie. Each embedding is the averaged vector of all word vectors in the movie's `text_corpus`.\n",
       "\n",
       "# Additional Notes:\n",
       "\n",
       "# - The implemented preprocessing steps, such as tokenization and stop words removal, are essential for both models to focus on the most meaningful words in the texts.\n",
       "# - Parameters for both TF-IDF and Word2Vec (such as `vector_size`, `window`, etc.) can be tuned according to specific requirements or based on experimentation to improve the performance of downstream tasks.\n",
       "# - For more accurate or contextual embeddings, consider additional preprocessing or feature engineering steps based on the dataset's characteristics and the target application.\n",
       "```\n",
       "\n",
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt2 = helper_extraction_prompt(msg_1)\n",
    "msg_2 = get_resp_oai(prompt2,\"gpt-4-0125-preview\")\n",
    "display(Code(msg_2, language='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21de9696-5c26-42a5-96e1-7c87ad52e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# To achieve the objective of creating embeddings for the given movie dataset using Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec (W2V) approaches, I'll walk you through both methods step by step, including the appropriate data preprocessing techniques.\\n\\n# 1. Data Preprocessing\\n\\n# Data preprocessing is crucial for both TF-IDF and Word2Vec models to ensure the models have clean and relevant data to work with. Given our dataset, we'll focus on text-based features such as `movie_title`, `original_title`, `tagline`, and `genres_x` as they are likely to provide the most meaningful context for generating embeddings.\\n\\n# First, let's prepare our environment and load the dataset:\\n\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom gensim.models import Word2Vec\\nimport string\\nimport re\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom sklearn.preprocessing import normalize\\nimport nltk\\n\\nnltk.download('punkt')\\nnltk.download('stopwords')\\n\\n# Load the dataset (assuming the file path is correct)\\ndataset_path = 'final_merged_df.csv' # Please replace this with your dataset path\\nmovies_dataset = pd.read_csv(dataset_path)\\n\\n# Preview the dataset\\nprint(movies_dataset.head())\\n\\n# Cleaning and Preparing Text Data\\n\\n# For both TF-IDF and Word2Vec models, our text data must be cleaned and preprocessed. This involves converting to lowercase, removing punctuation, stop words, and tokenization.\\n\\n# Function to clean text data\\ndef clean_text(text):\\n    if not pd.isnull(text):\\n        # Convert text to lowercase\\n        text = text.lower()\\n        # Remove numbers and punctuation\\n        text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n        # Tokenize the text\\n        tokens = word_tokenize(text)\\n        # Remove stop words\\n        stop_words = set(stopwords.words('english'))\\n        tokens = [w for w in tokens if not w in stop_words]\\n        # Join the tokens back into a string\\n        text = ' '.join(tokens)\\n    else:\\n        text = ''\\n    return text\\n\\n# Columns to preprocess\\ntext_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\\n\\n# Apply cleaning function to text columns\\nfor column in text_columns:\\n    movies_dataset[column] = movies_dataset[column].apply(clean_text)\\n\\n# Combine relevant text features into a single text corpus per movie\\nmovies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\\n\\n# 2. TF-IDF Embeddings\\n\\n# Now, let's create TF-IDF embeddings for our dataset.\\n\\n# Initialize TF-IDF Vectorizer\\ntfidf_vectorizer = TfidfVectorizer()\\n\\n# Generate TF-IDF embeddings\\ntfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\\n\\n# Normalize TF-IDF embeddings\\ntfidf_matrix = normalize(tfidf_matrix)\\n\\n# 3. Word2Vec Embeddings\\n\\n# For Word2Vec, we'll need to prepare our corpus slightly differently.\\n\\n# Prepare corpus for Word2Vec\\ncorpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\\n\\n# Initialize and train the Word2Vec model\\nword2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\\n\\n# Generate Word2Vec embeddings (averaged across all words in the document's corpus)\\nword2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\\n\\n# Deliverables\\n\\n# 1. **TF-IDF Embeddings:** The `tfidf_matrix` from the TF-IDF section contains the embeddings for each movie based on the `text_corpus`. You can save or use this matrix as needed.\\n\\n# 2. **Word2Vec Embeddings:** The `word2vec_embeddings` array from the Word2Vec section contains the embeddings for each movie. Each embedding is the averaged vector of all word vectors in the movie's `text_corpus`.\\n\\n# Additional Notes:\\n\\n# - The implemented preprocessing steps, such as tokenization and stop words removal, are essential for both models to focus on the most meaningful words in the texts.\\n# - Parameters for both TF-IDF and Word2Vec (such as `vector_size`, `window`, etc.) can be tuned according to specific requirements or based on experimentation to improve the performance of downstream tasks.\\n# - For more accurate or contextual embeddings, consider additional preprocessing or feature engineering steps based on the dataset's characteristics and the target application.\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the df name generated by the LLM with our dataframe path\n",
    "msg_2[15:len(msg_2)-8].replace('path_to_your_dataset.csv', 'final_merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510c9ab3-a745-4795-af2c-f0e1b96de989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/22/40/7d2cce3ad4ad5d02aa68e253e6ea5f0acc381f02f594e235fe00a274faff/gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "571302a6-ff78-491d-84a4-0f25b2db622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      director_name  duration      actor_2_name  \\\n",
      "0           0      James Cameron     178.0  Joel David Moore   \n",
      "1           1     Gore Verbinski     169.0     Orlando Bloom   \n",
      "2           2         Sam Mendes     148.0      Rory Kinnear   \n",
      "3           3  Christopher Nolan     164.0    Christian Bale   \n",
      "4           4     Andrew Stanton     132.0   Samantha Morton   \n",
      "\n",
      "                          genres_x     actor_1_name  \\\n",
      "0  Action|Adventure|Fantasy|Sci-Fi      CCH Pounder   \n",
      "1         Action|Adventure|Fantasy      Johnny Depp   \n",
      "2        Action|Adventure|Thriller  Christoph Waltz   \n",
      "3                  Action|Thriller        Tom Hardy   \n",
      "4          Action|Adventure|Sci-Fi     Daryl Sabara   \n",
      "\n",
      "                              movie_title          actor_3_name  \\\n",
      "0                                  Avatar             Wes Studi   \n",
      "1  Pirates of the Caribbean At Worlds End        Jack Davenport   \n",
      "2                                 Spectre      Stephanie Sigman   \n",
      "3                   The Dark Knight Rises  Joseph Gordon-Levitt   \n",
      "4                             John Carter          Polly Walker   \n",
      "\n",
      "                                     movie_imdb_link language  ...  \\\n",
      "0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...  English  ...   \n",
      "1  http://www.imdb.com/title/tt0449088/?ref_=fn_t...  English  ...   \n",
      "2  http://www.imdb.com/title/tt2379713/?ref_=fn_t...  English  ...   \n",
      "3  http://www.imdb.com/title/tt1345836/?ref_=fn_t...  English  ...   \n",
      "4  http://www.imdb.com/title/tt0401729/?ref_=fn_t...  English  ...   \n",
      "\n",
      "  Genre_Western  duration_category  rating_category           budget_category  \\\n",
      "0             0    165-180 Minutes              7-8  Greater than 200 Million   \n",
      "1             0    165-180 Minutes              7-8  Greater than 200 Million   \n",
      "2             0    135-150 Minutes              6-7  Greater than 200 Million   \n",
      "3             0    150-165 Minutes              8-9  Greater than 200 Million   \n",
      "4             0    120-135 Minutes              6-7  Greater than 200 Million   \n",
      "\n",
      "           revenue_category  budget_revenue_ratio log_budget  log_revenue  \\\n",
      "0  Greater than 1.8 Billion              0.085009  19.283571    21.748578   \n",
      "1          800-1000 Million              0.312176  19.519293    20.683485   \n",
      "2          800-1000 Million              0.278197  19.316769    20.596199   \n",
      "3            1-1.20 Billion              0.230429  19.336971    20.804790   \n",
      "4           200-400 Million              0.915046  19.376192    19.464974   \n",
      "\n",
      "   log_title_year  sqr_root_duration  \n",
      "0        7.605890          13.341664  \n",
      "1        7.604894          13.000000  \n",
      "2        7.608871          12.165525  \n",
      "3        7.607381          12.806248  \n",
      "4        7.607381          11.489125  \n",
      "\n",
      "[5 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "exec(msg_2[15:len(msg_2)-8].replace('path_to_your_dataset.csv', 'final_merged_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffaa7a67-e958-4f71-9578-15fcc53d7736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4144x7295 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c89845e-129f-4748-bfb2-73cc2fccd227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0062125 ,  0.00827003,  0.00314423, ..., -0.00956431,\n",
       "        -0.00091494,  0.00272677],\n",
       "       [-0.01262492,  0.01499987,  0.00747827, ..., -0.0206518 ,\n",
       "         0.00198918, -0.00159149],\n",
       "       [-0.01069544,  0.01570955,  0.0025968 , ..., -0.01611487,\n",
       "         0.00268714, -0.00023023],\n",
       "       ...,\n",
       "       [-0.00255603,  0.0029258 , -0.00304272, ..., -0.00521181,\n",
       "         0.00146588,  0.00587271],\n",
       "       [-0.00592796,  0.00554268,  0.00104809, ..., -0.00599817,\n",
       "         0.00168504,  0.00219389],\n",
       "       [-0.00652725,  0.00920803, -0.00099061, ..., -0.01675735,\n",
       "         0.0017123 , -0.00069297]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162aed5-8126-467e-8ab3-499d4a345d36",
   "metadata": {},
   "source": [
    "## Part II: Get movie recommendations based on similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fbca8b27-b183-455f-91ce-bfcb09db7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_extraction_prompt_rec(resp_str):\n",
    "    return f\"Can you transform this string: {resp_str} so that it calculates the cosine similarity for TF-IDF embeddings and Word2Vec? Please place comment symbols where necessary and keep in mind that the given response string will be executed in a python code chunk using the function:'exec(given_response_string).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dcd98319-b796-48da-8cb4-06b30c3f34e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"o\">---</span>\n",
       "\n",
       "<span class=\"n\">Certainly</span><span class=\"err\">!</span> <span class=\"n\">I</span><span class=\"s1\">&#39;ve modified your script to include the calculation of cosine similarity for both the TF-IDF and Word2Vec embeddings. Note the use of the cosine similarity functions from `sklearn.metrics.pairwise` for TF-IDF and manual implementation for Word2Vec (due to its nature of averaging vectors). Comments are appropriately placed to guide you through the additions. </span>\n",
       "\n",
       "<span class=\"n\">Please</span> <span class=\"n\">make</span> <span class=\"n\">sure</span> <span class=\"n\">to</span> <span class=\"n\">adjust</span> <span class=\"n\">the</span> <span class=\"n\">similarity</span> <span class=\"n\">calculations</span> <span class=\"n\">according</span> <span class=\"n\">to</span> <span class=\"n\">your</span> <span class=\"n\">specific</span> <span class=\"n\">needs</span><span class=\"p\">,</span> <span class=\"n\">especially</span> <span class=\"k\">if</span> <span class=\"n\">you</span><span class=\"s1\">&#39;re working with large datasets, as calculating similarities is memory intensive.</span>\n",
       "\n",
       "<span class=\"o\">---</span>\n",
       "\n",
       "<span class=\"err\">```</span><span class=\"n\">python</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">TfidfVectorizer</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">gensim.models</span> <span class=\"kn\">import</span> <span class=\"n\">Word2Vec</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">string</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">stopwords</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.tokenize</span> <span class=\"kn\">import</span> <span class=\"n\">word_tokenize</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">normalize</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics.pairwise</span> <span class=\"kn\">import</span> <span class=\"n\">cosine_similarity</span>\n",
       "\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;punkt&#39;</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;stopwords&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"n\">dataset_path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;path_to_your_dataset.csv&#39;</span>\n",
       "<span class=\"n\">movies_dataset</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">dataset_path</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">clean_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">isnull</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;[^\\w\\s]&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;&#39;</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">stop_words</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">stopwords</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s1\">&#39;english&#39;</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">stop_words</span><span class=\"p\">]</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">text</span>\n",
       "\n",
       "<span class=\"n\">text_columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"k\">for</span> <span class=\"n\">column</span> <span class=\"ow\">in</span> <span class=\"n\">text_columns</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">clean_text</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"n\">tfidf_vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">TfidfVectorizer</span><span class=\"p\">()</span>\n",
       "\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tfidf_vectorizer</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">])</span>\n",
       "\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">tfidf_matrix</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Calculate Cosine Similarity for TF-IDF</span>\n",
       "<span class=\"c1\"># This calculates the similarity between movies based on their TF-IDF embeddings</span>\n",
       "<span class=\"n\">tfidf_cosine_similarity</span> <span class=\"o\">=</span> <span class=\"n\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">tfidf_matrix</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]]</span>\n",
       "\n",
       "<span class=\"n\">word2vec_model</span> <span class=\"o\">=</span> <span class=\"n\">Word2Vec</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"o\">=</span><span class=\"n\">corpus</span><span class=\"p\">,</span> <span class=\"n\">vector_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">min_count</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"n\">word2vec_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">([</span><span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">document</span> <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">document</span> <span class=\"ow\">in</span> <span class=\"n\">corpus</span><span class=\"p\">])</span>\n",
       "\n",
       "<span class=\"c1\"># Calculate Cosine Similarity for Word2Vec manually as we have custom vectors</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">calculate_cosine_similarity_word2vec</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">similarity</span> <span class=\"o\">=</span> <span class=\"n\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">similarity</span>\n",
       "\n",
       "<span class=\"n\">word2vec_cosine_similarity</span> <span class=\"o\">=</span> <span class=\"n\">calculate_cosine_similarity_word2vec</span><span class=\"p\">(</span><span class=\"n\">word2vec_embeddings</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># You can use `tfidf_cosine_similarity` and `word2vec_cosine_similarity` as needed</span>\n",
       "<span class=\"err\">```</span>\n",
       "\n",
       "<span class=\"o\">---</span>\n",
       "<span class=\"n\">This</span> <span class=\"n\">code</span> <span class=\"n\">block</span> <span class=\"n\">introduces</span> <span class=\"n\">functions</span> <span class=\"n\">to</span> <span class=\"n\">calculate</span> <span class=\"n\">the</span> <span class=\"n\">cosine</span> <span class=\"n\">similarity</span> <span class=\"k\">for</span> <span class=\"n\">both</span> <span class=\"n\">TF</span><span class=\"o\">-</span><span class=\"n\">IDF</span> <span class=\"ow\">and</span> <span class=\"n\">Word2Vec</span> <span class=\"n\">embeddings</span><span class=\"o\">.</span> <span class=\"n\">For</span> <span class=\"n\">Word2Vec</span><span class=\"p\">,</span> <span class=\"n\">I</span> <span class=\"n\">added</span> <span class=\"n\">a</span> <span class=\"n\">helper</span> <span class=\"n\">function</span> <span class=\"err\">`</span><span class=\"n\">calculate_cosine_similarity_word2vec</span><span class=\"err\">`</span> <span class=\"n\">to</span> <span class=\"n\">handle</span> <span class=\"n\">the</span> <span class=\"n\">similarity</span> <span class=\"n\">calculations</span><span class=\"o\">.</span> <span class=\"n\">Remember</span> <span class=\"n\">to</span> <span class=\"n\">adapt</span> <span class=\"n\">the</span> <span class=\"n\">dataset</span> <span class=\"n\">path</span> <span class=\"ow\">and</span> <span class=\"n\">possibly</span> <span class=\"n\">tweak</span> <span class=\"n\">the</span> <span class=\"n\">Word2Vec</span> <span class=\"n\">parameters</span> <span class=\"ow\">or</span> <span class=\"n\">the</span> <span class=\"n\">pre</span><span class=\"o\">-</span><span class=\"n\">processing</span> <span class=\"n\">steps</span> <span class=\"k\">for</span> <span class=\"n\">better</span> <span class=\"n\">results</span> <span class=\"n\">tailored</span> <span class=\"n\">to</span> <span class=\"n\">your</span> <span class=\"n\">needs</span><span class=\"o\">.</span>\n",
       "\n",
       "<span class=\"o\">---</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\n",
       "\\PY{n}{Certainly}\\PY{err}{!} \\PY{n}{I}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ve modified your script to include the calculation of cosine similarity for both the TF\\PYZhy{}IDF and Word2Vec embeddings. Note the use of the cosine similarity functions from `sklearn.metrics.pairwise` for TF\\PYZhy{}IDF and manual implementation for Word2Vec (due to its nature of averaging vectors). Comments are appropriately placed to guide you through the additions. }\n",
       "\n",
       "\\PY{n}{Please} \\PY{n}{make} \\PY{n}{sure} \\PY{n}{to} \\PY{n}{adjust} \\PY{n}{the} \\PY{n}{similarity} \\PY{n}{calculations} \\PY{n}{according} \\PY{n}{to} \\PY{n}{your} \\PY{n}{specific} \\PY{n}{needs}\\PY{p}{,} \\PY{n}{especially} \\PY{k}{if} \\PY{n}{you}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{re working with large datasets, as calculating similarities is memory intensive.}\n",
       "\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\\PY{n}{python}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{pandas} \\PY{k}{as} \\PY{n+nn}{pd}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{feature\\PYZus{}extraction}\\PY{n+nn}{.}\\PY{n+nn}{text} \\PY{k+kn}{import} \\PY{n}{TfidfVectorizer}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{gensim}\\PY{n+nn}{.}\\PY{n+nn}{models} \\PY{k+kn}{import} \\PY{n}{Word2Vec}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{string}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{re}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{corpus} \\PY{k+kn}{import} \\PY{n}{stopwords}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{tokenize} \\PY{k+kn}{import} \\PY{n}{word\\PYZus{}tokenize}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{preprocessing} \\PY{k+kn}{import} \\PY{n}{normalize}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{nltk}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{metrics}\\PY{n+nn}{.}\\PY{n+nn}{pairwise} \\PY{k+kn}{import} \\PY{n}{cosine\\PYZus{}similarity}\n",
       "\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{punkt}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{stopwords}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{dataset\\PYZus{}path} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{path\\PYZus{}to\\PYZus{}your\\PYZus{}dataset.csv}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "\\PY{n}{movies\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{read\\PYZus{}csv}\\PY{p}{(}\\PY{n}{dataset\\PYZus{}path}\\PY{p}{)}\n",
       "\n",
       "\\PY{n+nb}{print}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{o}{.}\\PY{n}{head}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{clean\\PYZus{}text}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{isnull}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{text}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{re}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+s+sa}{r}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{[\\PYZca{}}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{w}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{s]}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{n}{stop\\PYZus{}words} \\PY{o}{=} \\PY{n+nb}{set}\\PY{p}{(}\\PY{n}{stopwords}\\PY{o}{.}\\PY{n}{words}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{english}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{p}{[}\\PY{n}{w} \\PY{k}{for} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{tokens} \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{stop\\PYZus{}words}\\PY{p}{]}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{tokens}\\PY{p}{)}\n",
       "    \\PY{k}{else}\\PY{p}{:}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "    \\PY{k}{return} \\PY{n}{text}\n",
       "\n",
       "\\PY{n}{text\\PYZus{}columns} \\PY{o}{=} \\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{k}{for} \\PY{n}{column} \\PY{o+ow}{in} \\PY{n}{text\\PYZus{}columns}\\PY{p}{:}\n",
       "    \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]}\\PY{o}{.}\\PY{n}{apply}\\PY{p}{(}\\PY{n}{clean\\PYZus{}text}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{n}{tfidf\\PYZus{}vectorizer} \\PY{o}{=} \\PY{n}{TfidfVectorizer}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{tfidf\\PYZus{}vectorizer}\\PY{o}{.}\\PY{n}{fit\\PYZus{}transform}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{normalize}\\PY{p}{(}\\PY{n}{tfidf\\PYZus{}matrix}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Calculate Cosine Similarity for TF\\PYZhy{}IDF}\n",
       "\\PY{c+c1}{\\PYZsh{} This calculates the similarity between movies based on their TF\\PYZhy{}IDF embeddings}\n",
       "\\PY{n}{tfidf\\PYZus{}cosine\\PYZus{}similarity} \\PY{o}{=} \\PY{n}{cosine\\PYZus{}similarity}\\PY{p}{(}\\PY{n}{tfidf\\PYZus{}matrix}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{corpus} \\PY{o}{=} \\PY{p}{[}\\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{doc}\\PY{p}{)} \\PY{k}{for} \\PY{n}{doc} \\PY{o+ow}{in} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{]}\n",
       "\n",
       "\\PY{n}{word2vec\\PYZus{}model} \\PY{o}{=} \\PY{n}{Word2Vec}\\PY{p}{(}\\PY{n}{sentences}\\PY{o}{=}\\PY{n}{corpus}\\PY{p}{,} \\PY{n}{vector\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{100}\\PY{p}{,} \\PY{n}{window}\\PY{o}{=}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{n}{min\\PYZus{}count}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{word2vec\\PYZus{}embeddings} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{array}\\PY{p}{(}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{p}{[}\\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{[}\\PY{n}{word}\\PY{p}{]} \\PY{k}{for} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{document} \\PY{k}{if} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{]} \\PY{o+ow}{or} \\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{zeros}\\PY{p}{(}\\PY{l+m+mi}{100}\\PY{p}{)}\\PY{p}{]}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{0}\\PY{p}{)} \\PY{k}{for} \\PY{n}{document} \\PY{o+ow}{in} \\PY{n}{corpus}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Calculate Cosine Similarity for Word2Vec manually as we have custom vectors}\n",
       "\\PY{k}{def} \\PY{n+nf}{calculate\\PYZus{}cosine\\PYZus{}similarity\\PYZus{}word2vec}\\PY{p}{(}\\PY{n}{vectors}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{similarity} \\PY{o}{=} \\PY{n}{cosine\\PYZus{}similarity}\\PY{p}{(}\\PY{n}{vectors}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{similarity}\n",
       "\n",
       "\\PY{n}{word2vec\\PYZus{}cosine\\PYZus{}similarity} \\PY{o}{=} \\PY{n}{calculate\\PYZus{}cosine\\PYZus{}similarity\\PYZus{}word2vec}\\PY{p}{(}\\PY{n}{word2vec\\PYZus{}embeddings}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} You can use `tfidf\\PYZus{}cosine\\PYZus{}similarity` and `word2vec\\PYZus{}cosine\\PYZus{}similarity` as needed}\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\n",
       "\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\\PY{n}{This} \\PY{n}{code} \\PY{n}{block} \\PY{n}{introduces} \\PY{n}{functions} \\PY{n}{to} \\PY{n}{calculate} \\PY{n}{the} \\PY{n}{cosine} \\PY{n}{similarity} \\PY{k}{for} \\PY{n}{both} \\PY{n}{TF}\\PY{o}{\\PYZhy{}}\\PY{n}{IDF} \\PY{o+ow}{and} \\PY{n}{Word2Vec} \\PY{n}{embeddings}\\PY{o}{.} \\PY{n}{For} \\PY{n}{Word2Vec}\\PY{p}{,} \\PY{n}{I} \\PY{n}{added} \\PY{n}{a} \\PY{n}{helper} \\PY{n}{function} \\PY{err}{`}\\PY{n}{calculate\\PYZus{}cosine\\PYZus{}similarity\\PYZus{}word2vec}\\PY{err}{`} \\PY{n}{to} \\PY{n}{handle} \\PY{n}{the} \\PY{n}{similarity} \\PY{n}{calculations}\\PY{o}{.} \\PY{n}{Remember} \\PY{n}{to} \\PY{n}{adapt} \\PY{n}{the} \\PY{n}{dataset} \\PY{n}{path} \\PY{o+ow}{and} \\PY{n}{possibly} \\PY{n}{tweak} \\PY{n}{the} \\PY{n}{Word2Vec} \\PY{n}{parameters} \\PY{o+ow}{or} \\PY{n}{the} \\PY{n}{pre}\\PY{o}{\\PYZhy{}}\\PY{n}{processing} \\PY{n}{steps} \\PY{k}{for} \\PY{n}{better} \\PY{n}{results} \\PY{n}{tailored} \\PY{n}{to} \\PY{n}{your} \\PY{n}{needs}\\PY{o}{.}\n",
       "\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "Certainly! I've modified your script to include the calculation of cosine similarity for both the TF-IDF and Word2Vec embeddings. Note the use of the cosine similarity functions from `sklearn.metrics.pairwise` for TF-IDF and manual implementation for Word2Vec (due to its nature of averaging vectors). Comments are appropriately placed to guide you through the additions. \n",
       "\n",
       "Please make sure to adjust the similarity calculations according to your specific needs, especially if you're working with large datasets, as calculating similarities is memory intensive.\n",
       "\n",
       "---\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "from gensim.models import Word2Vec\n",
       "import string\n",
       "import re\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.tokenize import word_tokenize\n",
       "from sklearn.preprocessing import normalize\n",
       "import nltk\n",
       "from sklearn.metrics.pairwise import cosine_similarity\n",
       "\n",
       "nltk.download('punkt')\n",
       "nltk.download('stopwords')\n",
       "\n",
       "dataset_path = 'path_to_your_dataset.csv'\n",
       "movies_dataset = pd.read_csv(dataset_path)\n",
       "\n",
       "print(movies_dataset.head())\n",
       "\n",
       "def clean_text(text):\n",
       "    if not pd.isnull(text):\n",
       "        text = text.lower()\n",
       "        text = re.sub(r'[^\\w\\s]', '', text)\n",
       "        tokens = word_tokenize(text)\n",
       "        stop_words = set(stopwords.words('english'))\n",
       "        tokens = [w for w in tokens if not w in stop_words]\n",
       "        text = ' '.join(tokens)\n",
       "    else:\n",
       "        text = ''\n",
       "    return text\n",
       "\n",
       "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
       "\n",
       "for column in text_columns:\n",
       "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
       "\n",
       "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
       "\n",
       "tfidf_vectorizer = TfidfVectorizer()\n",
       "\n",
       "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\n",
       "\n",
       "tfidf_matrix = normalize(tfidf_matrix)\n",
       "\n",
       "# Calculate Cosine Similarity for TF-IDF\n",
       "# This calculates the similarity between movies based on their TF-IDF embeddings\n",
       "tfidf_cosine_similarity = cosine_similarity(tfidf_matrix)\n",
       "\n",
       "corpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\n",
       "\n",
       "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "word2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\n",
       "\n",
       "# Calculate Cosine Similarity for Word2Vec manually as we have custom vectors\n",
       "def calculate_cosine_similarity_word2vec(vectors):\n",
       "    similarity = cosine_similarity(vectors)\n",
       "    return similarity\n",
       "\n",
       "word2vec_cosine_similarity = calculate_cosine_similarity_word2vec(word2vec_embeddings)\n",
       "\n",
       "# You can use `tfidf_cosine_similarity` and `word2vec_cosine_similarity` as needed\n",
       "```\n",
       "\n",
       "---\n",
       "This code block introduces functions to calculate the cosine similarity for both TF-IDF and Word2Vec embeddings. For Word2Vec, I added a helper function `calculate_cosine_similarity_word2vec` to handle the similarity calculations. Remember to adapt the dataset path and possibly tweak the Word2Vec parameters or the pre-processing steps for better results tailored to your needs.\n",
       "\n",
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt3 = helper_extraction_prompt_rec(msg_2)\n",
    "msg_3 = get_resp_oai(prompt3,\"gpt-4-0125-preview\")\n",
    "display(Code(msg_3, language ='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f8f734b3-8d3e-498c-8346-e9ce5cb19ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_extraction_prompt_rec(resp_str):\n",
    "    return f\"Can you transform this string: {resp_str} so that it also finds the indices of the top 20 similar movies based on a 'movie_tile'? Please extract all code and place comment symbols where necessary keeping in mind that the given response string will be executed in a python code chunk using the function: 'exec(given_response_string). Only provide the code chunk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c6cfbe9f-a390-4b13-afb5-3b5fb9b43791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"o\">---</span>\n",
       "\n",
       "<span class=\"err\">```</span><span class=\"n\">python</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">TfidfVectorizer</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">gensim.models</span> <span class=\"kn\">import</span> <span class=\"n\">Word2Vec</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">string</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">stopwords</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nltk.tokenize</span> <span class=\"kn\">import</span> <span class=\"n\">word_tokenize</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">normalize</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics.pairwise</span> <span class=\"kn\">import</span> <span class=\"n\">cosine_similarity</span>\n",
       "\n",
       "<span class=\"c1\"># Download necessary NLTK data</span>\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;punkt&#39;</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">&#39;stopwords&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Load the dataset</span>\n",
       "<span class=\"n\">dataset_path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;path_to_your_dataset.csv&#39;</span>\n",
       "<span class=\"n\">movies_dataset</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">dataset_path</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n",
       "\n",
       "<span class=\"c1\"># Function to clean text data</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">clean_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">isnull</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;[^\\w\\s]&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;&#39;</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">stop_words</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">stopwords</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s1\">&#39;english&#39;</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">stop_words</span><span class=\"p\">]</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">text</span>\n",
       "\n",
       "<span class=\"c1\"># Columns containing textual information</span>\n",
       "<span class=\"n\">text_columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"c1\"># Clean text columns</span>\n",
       "<span class=\"k\">for</span> <span class=\"n\">column</span> <span class=\"ow\">in</span> <span class=\"n\">text_columns</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">column</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">clean_text</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Combine text columns into a single corpus</span>\n",
       "<span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;original_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;tagline&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39; &#39;</span> <span class=\"o\">+</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;genres_x&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"c1\"># Initialize TF-IDF Vectorizer and Word2Vec model</span>\n",
       "<span class=\"n\">tfidf_vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">TfidfVectorizer</span><span class=\"p\">()</span>\n",
       "\n",
       "<span class=\"c1\"># Generate TF-IDF features</span>\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tfidf_vectorizer</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">])</span>\n",
       "<span class=\"n\">tfidf_matrix</span> <span class=\"o\">=</span> <span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">tfidf_matrix</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Calculate Cosine Similarity for TF-IDF</span>\n",
       "<span class=\"n\">tfidf_cosine_similarity</span> <span class=\"o\">=</span> <span class=\"n\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">tfidf_matrix</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Prepare corpus for Word2Vec</span>\n",
       "<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;text_corpus&#39;</span><span class=\"p\">]]</span>\n",
       "<span class=\"n\">word2vec_model</span> <span class=\"o\">=</span> <span class=\"n\">Word2Vec</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"o\">=</span><span class=\"n\">corpus</span><span class=\"p\">,</span> <span class=\"n\">vector_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">min_count</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Generate Word2Vec features</span>\n",
       "<span class=\"n\">word2vec_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">([</span><span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">document</span> <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">word2vec_model</span><span class=\"o\">.</span><span class=\"n\">wv</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">document</span> <span class=\"ow\">in</span> <span class=\"n\">corpus</span><span class=\"p\">])</span>\n",
       "\n",
       "<span class=\"c1\"># Define function to calculate cosine similarity for Word2Vec</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">calculate_cosine_similarity_word2vec</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">similarity</span> <span class=\"o\">=</span> <span class=\"n\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">similarity</span>\n",
       "\n",
       "<span class=\"c1\"># Calculate Cosine Similarity for Word2Vec</span>\n",
       "<span class=\"n\">word2vec_cosine_similarity</span> <span class=\"o\">=</span> <span class=\"n\">calculate_cosine_similarity_word2vec</span><span class=\"p\">(</span><span class=\"n\">word2vec_embeddings</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Function to find top N similar movies based on movie title using TF-IDF or Word2Vec similarity</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">top_n_similar_movies</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">use_tfidf</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\"># Find index of the movie title</span>\n",
       "    <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">title</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
       "\n",
       "    <span class=\"c1\"># Choose similarity matrix</span>\n",
       "    <span class=\"n\">similarity_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tfidf_cosine_similarity</span> <span class=\"k\">if</span> <span class=\"n\">use_tfidf</span> <span class=\"k\">else</span> <span class=\"n\">word2vec_cosine_similarity</span>\n",
       "\n",
       "    <span class=\"c1\"># Get similarity values with other movies</span>\n",
       "    <span class=\"c1\"># Skip the movie itself</span>\n",
       "    <span class=\"n\">similarity_scores</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">similarity_matrix</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]))</span>\n",
       "    <span class=\"n\">similarity_scores</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">similarity_scores</span> <span class=\"k\">if</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"n\">idx</span><span class=\"p\">]</span>\n",
       "\n",
       "    <span class=\"c1\"># Sort movies based on similarity scores</span>\n",
       "    <span class=\"n\">similarity_scores</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">similarity_scores</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Get the scores of the top N most similar movies</span>\n",
       "    <span class=\"n\">top_n_similar_indices</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">similarity_scores</span><span class=\"p\">[:</span><span class=\"n\">n</span><span class=\"p\">]]</span>\n",
       "\n",
       "    <span class=\"c1\"># Return the top N similar movies</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">movies_dataset</span><span class=\"p\">[</span><span class=\"s1\">&#39;movie_title&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">iloc</span><span class=\"p\">[</span><span class=\"n\">top_n_similar_indices</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"c1\"># Example to find top 20 similar movies for a given title</span>\n",
       "<span class=\"n\">top_similar_movies</span> <span class=\"o\">=</span> <span class=\"n\">top_n_similar_movies</span><span class=\"p\">(</span><span class=\"s1\">&#39;example_movie_title&#39;</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">use_tfidf</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">top_similar_movies</span><span class=\"p\">)</span>\n",
       "<span class=\"err\">```</span>\n",
       "\n",
       "<span class=\"o\">---</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\\PY{n}{python}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{pandas} \\PY{k}{as} \\PY{n+nn}{pd}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{feature\\PYZus{}extraction}\\PY{n+nn}{.}\\PY{n+nn}{text} \\PY{k+kn}{import} \\PY{n}{TfidfVectorizer}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{gensim}\\PY{n+nn}{.}\\PY{n+nn}{models} \\PY{k+kn}{import} \\PY{n}{Word2Vec}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{string}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{re}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{corpus} \\PY{k+kn}{import} \\PY{n}{stopwords}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nltk}\\PY{n+nn}{.}\\PY{n+nn}{tokenize} \\PY{k+kn}{import} \\PY{n}{word\\PYZus{}tokenize}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{preprocessing} \\PY{k+kn}{import} \\PY{n}{normalize}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{nltk}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{metrics}\\PY{n+nn}{.}\\PY{n+nn}{pairwise} \\PY{k+kn}{import} \\PY{n}{cosine\\PYZus{}similarity}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Download necessary NLTK data}\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{punkt}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\\PY{n}{nltk}\\PY{o}{.}\\PY{n}{download}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{stopwords}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Load the dataset}\n",
       "\\PY{n}{dataset\\PYZus{}path} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{path\\PYZus{}to\\PYZus{}your\\PYZus{}dataset.csv}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "\\PY{n}{movies\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{read\\PYZus{}csv}\\PY{p}{(}\\PY{n}{dataset\\PYZus{}path}\\PY{p}{)}\n",
       "\n",
       "\\PY{n+nb}{print}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{o}{.}\\PY{n}{head}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Function to clean text data}\n",
       "\\PY{k}{def} \\PY{n+nf}{clean\\PYZus{}text}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{isnull}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{text}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{n}{re}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+s+sa}{r}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{[\\PYZca{}}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{w}\\PY{l+s+s1}{\\PYZbs{}}\\PY{l+s+s1}{s]}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{text}\\PY{p}{)}\n",
       "        \\PY{n}{stop\\PYZus{}words} \\PY{o}{=} \\PY{n+nb}{set}\\PY{p}{(}\\PY{n}{stopwords}\\PY{o}{.}\\PY{n}{words}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{english}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{tokens} \\PY{o}{=} \\PY{p}{[}\\PY{n}{w} \\PY{k}{for} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{tokens} \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{w} \\PY{o+ow}{in} \\PY{n}{stop\\PYZus{}words}\\PY{p}{]}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{tokens}\\PY{p}{)}\n",
       "    \\PY{k}{else}\\PY{p}{:}\n",
       "        \\PY{n}{text} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "    \\PY{k}{return} \\PY{n}{text}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Columns containing textual information}\n",
       "\\PY{n}{text\\PYZus{}columns} \\PY{o}{=} \\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Clean text columns}\n",
       "\\PY{k}{for} \\PY{n}{column} \\PY{o+ow}{in} \\PY{n}{text\\PYZus{}columns}\\PY{p}{:}\n",
       "    \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{column}\\PY{p}{]}\\PY{o}{.}\\PY{n}{apply}\\PY{p}{(}\\PY{n}{clean\\PYZus{}text}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Combine text columns into a single corpus}\n",
       "\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{original\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{tagline}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{+} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{ }\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{+} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{genres\\PYZus{}x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Initialize TF\\PYZhy{}IDF Vectorizer and Word2Vec model}\n",
       "\\PY{n}{tfidf\\PYZus{}vectorizer} \\PY{o}{=} \\PY{n}{TfidfVectorizer}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Generate TF\\PYZhy{}IDF features}\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{tfidf\\PYZus{}vectorizer}\\PY{o}{.}\\PY{n}{fit\\PYZus{}transform}\\PY{p}{(}\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\\PY{n}{tfidf\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{normalize}\\PY{p}{(}\\PY{n}{tfidf\\PYZus{}matrix}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Calculate Cosine Similarity for TF\\PYZhy{}IDF}\n",
       "\\PY{n}{tfidf\\PYZus{}cosine\\PYZus{}similarity} \\PY{o}{=} \\PY{n}{cosine\\PYZus{}similarity}\\PY{p}{(}\\PY{n}{tfidf\\PYZus{}matrix}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Prepare corpus for Word2Vec}\n",
       "\\PY{n}{corpus} \\PY{o}{=} \\PY{p}{[}\\PY{n}{word\\PYZus{}tokenize}\\PY{p}{(}\\PY{n}{doc}\\PY{p}{)} \\PY{k}{for} \\PY{n}{doc} \\PY{o+ow}{in} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{text\\PYZus{}corpus}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{]}\n",
       "\\PY{n}{word2vec\\PYZus{}model} \\PY{o}{=} \\PY{n}{Word2Vec}\\PY{p}{(}\\PY{n}{sentences}\\PY{o}{=}\\PY{n}{corpus}\\PY{p}{,} \\PY{n}{vector\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{100}\\PY{p}{,} \\PY{n}{window}\\PY{o}{=}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{n}{min\\PYZus{}count}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Generate Word2Vec features}\n",
       "\\PY{n}{word2vec\\PYZus{}embeddings} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{array}\\PY{p}{(}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{p}{[}\\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{[}\\PY{n}{word}\\PY{p}{]} \\PY{k}{for} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{document} \\PY{k}{if} \\PY{n}{word} \\PY{o+ow}{in} \\PY{n}{word2vec\\PYZus{}model}\\PY{o}{.}\\PY{n}{wv}\\PY{p}{]} \\PY{o+ow}{or} \\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{zeros}\\PY{p}{(}\\PY{l+m+mi}{100}\\PY{p}{)}\\PY{p}{]}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{0}\\PY{p}{)} \\PY{k}{for} \\PY{n}{document} \\PY{o+ow}{in} \\PY{n}{corpus}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Define function to calculate cosine similarity for Word2Vec}\n",
       "\\PY{k}{def} \\PY{n+nf}{calculate\\PYZus{}cosine\\PYZus{}similarity\\PYZus{}word2vec}\\PY{p}{(}\\PY{n}{vectors}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{similarity} \\PY{o}{=} \\PY{n}{cosine\\PYZus{}similarity}\\PY{p}{(}\\PY{n}{vectors}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{similarity}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Calculate Cosine Similarity for Word2Vec}\n",
       "\\PY{n}{word2vec\\PYZus{}cosine\\PYZus{}similarity} \\PY{o}{=} \\PY{n}{calculate\\PYZus{}cosine\\PYZus{}similarity\\PYZus{}word2vec}\\PY{p}{(}\\PY{n}{word2vec\\PYZus{}embeddings}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Function to find top N similar movies based on movie title using TF\\PYZhy{}IDF or Word2Vec similarity}\n",
       "\\PY{k}{def} \\PY{n+nf}{top\\PYZus{}n\\PYZus{}similar\\PYZus{}movies}\\PY{p}{(}\\PY{n}{title}\\PY{p}{,} \\PY{n}{n}\\PY{o}{=}\\PY{l+m+mi}{20}\\PY{p}{,} \\PY{n}{use\\PYZus{}tfidf}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Find index of the movie title}\n",
       "    \\PY{n}{idx} \\PY{o}{=} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{==} \\PY{n}{title}\\PY{p}{]}\\PY{o}{.}\\PY{n}{index}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Choose similarity matrix}\n",
       "    \\PY{n}{similarity\\PYZus{}matrix} \\PY{o}{=} \\PY{n}{tfidf\\PYZus{}cosine\\PYZus{}similarity} \\PY{k}{if} \\PY{n}{use\\PYZus{}tfidf} \\PY{k}{else} \\PY{n}{word2vec\\PYZus{}cosine\\PYZus{}similarity}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Get similarity values with other movies}\n",
       "    \\PY{c+c1}{\\PYZsh{} Skip the movie itself}\n",
       "    \\PY{n}{similarity\\PYZus{}scores} \\PY{o}{=} \\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{similarity\\PYZus{}matrix}\\PY{p}{[}\\PY{n}{idx}\\PY{p}{]}\\PY{p}{)}\\PY{p}{)}\n",
       "    \\PY{n}{similarity\\PYZus{}scores} \\PY{o}{=} \\PY{p}{[}\\PY{n}{x} \\PY{k}{for} \\PY{n}{x} \\PY{o+ow}{in} \\PY{n}{similarity\\PYZus{}scores} \\PY{k}{if} \\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]} \\PY{o}{!=} \\PY{n}{idx}\\PY{p}{]}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Sort movies based on similarity scores}\n",
       "    \\PY{n}{similarity\\PYZus{}scores} \\PY{o}{=} \\PY{n+nb}{sorted}\\PY{p}{(}\\PY{n}{similarity\\PYZus{}scores}\\PY{p}{,} \\PY{n}{key}\\PY{o}{=}\\PY{k}{lambda} \\PY{n}{x}\\PY{p}{:} \\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{n}{reverse}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Get the scores of the top N most similar movies}\n",
       "    \\PY{n}{top\\PYZus{}n\\PYZus{}similar\\PYZus{}indices} \\PY{o}{=} \\PY{p}{[}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]} \\PY{k}{for} \\PY{n}{x} \\PY{o+ow}{in} \\PY{n}{similarity\\PYZus{}scores}\\PY{p}{[}\\PY{p}{:}\\PY{n}{n}\\PY{p}{]}\\PY{p}{]}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Return the top N similar movies}\n",
       "    \\PY{k}{return} \\PY{n}{movies\\PYZus{}dataset}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{iloc}\\PY{p}{[}\\PY{n}{top\\PYZus{}n\\PYZus{}similar\\PYZus{}indices}\\PY{p}{]}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Example to find top 20 similar movies for a given title}\n",
       "\\PY{n}{top\\PYZus{}similar\\PYZus{}movies} \\PY{o}{=} \\PY{n}{top\\PYZus{}n\\PYZus{}similar\\PYZus{}movies}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{example\\PYZus{}movie\\PYZus{}title}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+m+mi}{20}\\PY{p}{,} \\PY{n}{use\\PYZus{}tfidf}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "\\PY{n+nb}{print}\\PY{p}{(}\\PY{n}{top\\PYZus{}similar\\PYZus{}movies}\\PY{p}{)}\n",
       "\\PY{err}{`}\\PY{err}{`}\\PY{err}{`}\n",
       "\n",
       "\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZhy{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "from gensim.models import Word2Vec\n",
       "import string\n",
       "import re\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.tokenize import word_tokenize\n",
       "from sklearn.preprocessing import normalize\n",
       "import nltk\n",
       "from sklearn.metrics.pairwise import cosine_similarity\n",
       "\n",
       "# Download necessary NLTK data\n",
       "nltk.download('punkt')\n",
       "nltk.download('stopwords')\n",
       "\n",
       "# Load the dataset\n",
       "dataset_path = 'path_to_your_dataset.csv'\n",
       "movies_dataset = pd.read_csv(dataset_path)\n",
       "\n",
       "print(movies_dataset.head())\n",
       "\n",
       "# Function to clean text data\n",
       "def clean_text(text):\n",
       "    if not pd.isnull(text):\n",
       "        text = text.lower()\n",
       "        text = re.sub(r'[^\\w\\s]', '', text)\n",
       "        tokens = word_tokenize(text)\n",
       "        stop_words = set(stopwords.words('english'))\n",
       "        tokens = [w for w in tokens if not w in stop_words]\n",
       "        text = ' '.join(tokens)\n",
       "    else:\n",
       "        text = ''\n",
       "    return text\n",
       "\n",
       "# Columns containing textual information\n",
       "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
       "\n",
       "# Clean text columns\n",
       "for column in text_columns:\n",
       "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
       "\n",
       "# Combine text columns into a single corpus\n",
       "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
       "\n",
       "# Initialize TF-IDF Vectorizer and Word2Vec model\n",
       "tfidf_vectorizer = TfidfVectorizer()\n",
       "\n",
       "# Generate TF-IDF features\n",
       "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\n",
       "tfidf_matrix = normalize(tfidf_matrix)\n",
       "\n",
       "# Calculate Cosine Similarity for TF-IDF\n",
       "tfidf_cosine_similarity = cosine_similarity(tfidf_matrix)\n",
       "\n",
       "# Prepare corpus for Word2Vec\n",
       "corpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\n",
       "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "# Generate Word2Vec features\n",
       "word2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\n",
       "\n",
       "# Define function to calculate cosine similarity for Word2Vec\n",
       "def calculate_cosine_similarity_word2vec(vectors):\n",
       "    similarity = cosine_similarity(vectors)\n",
       "    return similarity\n",
       "\n",
       "# Calculate Cosine Similarity for Word2Vec\n",
       "word2vec_cosine_similarity = calculate_cosine_similarity_word2vec(word2vec_embeddings)\n",
       "\n",
       "# Function to find top N similar movies based on movie title using TF-IDF or Word2Vec similarity\n",
       "def top_n_similar_movies(title, n=20, use_tfidf=True):\n",
       "    # Find index of the movie title\n",
       "    idx = movies_dataset[movies_dataset['movie_title'] == title].index[0]\n",
       "\n",
       "    # Choose similarity matrix\n",
       "    similarity_matrix = tfidf_cosine_similarity if use_tfidf else word2vec_cosine_similarity\n",
       "\n",
       "    # Get similarity values with other movies\n",
       "    # Skip the movie itself\n",
       "    similarity_scores = list(enumerate(similarity_matrix[idx]))\n",
       "    similarity_scores = [x for x in similarity_scores if x[0] != idx]\n",
       "\n",
       "    # Sort movies based on similarity scores\n",
       "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
       "\n",
       "    # Get the scores of the top N most similar movies\n",
       "    top_n_similar_indices = [x[0] for x in similarity_scores[:n]]\n",
       "\n",
       "    # Return the top N similar movies\n",
       "    return movies_dataset['movie_title'].iloc[top_n_similar_indices]\n",
       "\n",
       "# Example to find top 20 similar movies for a given title\n",
       "top_similar_movies = top_n_similar_movies('example_movie_title', 20, use_tfidf=True)\n",
       "print(top_similar_movies)\n",
       "```\n",
       "\n",
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt31 = helper_extraction_prompt_rec(msg_3)\n",
    "msg_31 = get_resp_oai(prompt31,\"gpt-4-0125-preview\")\n",
    "display(Code(msg_31, language ='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddc6cc5-d717-4af6-a4ea-1dcc7bbf59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_311 = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import normalize\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'path_to_your_dataset.csv'\n",
    "movies_dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "print(movies_dataset.head())\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    if not pd.isnull(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        text = ' '.join(tokens)\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "# Columns containing textual information\n",
    "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
    "\n",
    "# Clean text columns\n",
    "for column in text_columns:\n",
    "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
    "\n",
    "# Combine text columns into a single corpus\n",
    "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer and Word2Vec model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_dataset['text_corpus'])\n",
    "tfidf_matrix = normalize(tfidf_matrix)\n",
    "\n",
    "# Calculate Cosine Similarity for TF-IDF\n",
    "tfidf_cosine_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Prepare corpus for Word2Vec\n",
    "corpus = [word_tokenize(doc) for doc in movies_dataset['text_corpus']]\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate Word2Vec features\n",
    "word2vec_embeddings = np.array([np.mean([word2vec_model.wv[word] for word in document if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for document in corpus])\n",
    "\n",
    "# Define function to calculate cosine similarity for Word2Vec\n",
    "def calculate_cosine_similarity_word2vec(vectors):\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity\n",
    "\n",
    "# Calculate Cosine Similarity for Word2Vec\n",
    "word2vec_cosine_similarity = calculate_cosine_similarity_word2vec(word2vec_embeddings)\n",
    "\n",
    "# Function to find top N similar movies based on movie title using TF-IDF or Word2Vec similarity\n",
    "def top_n_similar_movies(title, n=20, use_tfidf=True):\n",
    "    # Find index of the movie title\n",
    "    idx = movies_dataset[movies_dataset['movie_title'] == title].index[0]\n",
    "\n",
    "    # Choose similarity matrix\n",
    "    similarity_matrix = tfidf_cosine_similarity if use_tfidf else word2vec_cosine_similarity\n",
    "\n",
    "    # Get similarity values with other movies\n",
    "    # Skip the movie itself\n",
    "    similarity_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    similarity_scores = [x for x in similarity_scores if x[0] != idx]\n",
    "\n",
    "    # Sort movies based on similarity scores\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the top N most similar movies\n",
    "    top_n_similar_indices = [x[0] for x in similarity_scores[:n]]\n",
    "\n",
    "    # Return the top N similar movies\n",
    "    return movies_dataset['movie_title'].iloc[top_n_similar_indices]\n",
    "\n",
    "# Example to find top 20 similar movies for a given title\n",
    "top_similar_movies = top_n_similar_movies('example_movie_title', 20, use_tfidf=True)\n",
    "print(top_similar_movies)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ba8d143-cd2b-4db3-8c13-a20157409ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace1 = \"\"\"\n",
    "# Columns containing textual information\n",
    "text_columns = ['movie_title', 'original_title', 'tagline', 'genres_x']\n",
    "\n",
    "# Clean text columns\n",
    "for column in text_columns:\n",
    "    movies_dataset[column] = movies_dataset[column].apply(clean_text)\n",
    "\n",
    "# Combine text columns into a single corpus\n",
    "movies_dataset['text_corpus'] = movies_dataset['movie_title'] + ' ' + movies_dataset['original_title'] + ' ' + movies_dataset['tagline'] + ' ' + movies_dataset['genres_x']\n",
    "\"\"\"\n",
    "replacement1 = \"\"\"\n",
    "# Columns containing textual information\n",
    "text_columns =['director_name', 'actor_2_name', 'genres_x', 'actor_1_name',\n",
    "       'movie_title', 'actor_3_name', 'movie_imdb_link', 'language', 'country',\n",
    "       'original_title', 'tagline', 'duration_category', 'budget_category',\n",
    "       'revenue_category']\n",
    "\n",
    "movies_dataset['text_corpus'] = \"\"\n",
    "# Clean text columns\n",
    "for column in text_columns:\n",
    "    movies_dataset['text_corpus'] += movies_dataset[column].apply(clean_text) + ' '\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67482b5f-f040-45d0-b934-617e0901d4a3",
   "metadata": {},
   "source": [
    "Human Intervention required to get the code to run sucessfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a654381-c5e5-45ef-8e8c-c9dccb6b17d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25                                     Titanic\n",
      "96            The Hobbit An Unexpected Journey\n",
      "16                                The Avengers\n",
      "1574                             The New World\n",
      "24                                   King Kong\n",
      "14                                Man of Steel\n",
      "10                            Superman Returns\n",
      "3788                                   Hatchet\n",
      "3                        The Dark Knight Rises\n",
      "1932                                 RoboCop 3\n",
      "5                                  SpiderMan 3\n",
      "2002                                    Orphan\n",
      "4                                  John Carter\n",
      "429                                    FaceOff\n",
      "1       Pirates of the Caribbean At Worlds End\n",
      "288                                End of Days\n",
      "9            Batman v Superman Dawn of Justice\n",
      "3592                  Enter the Dangerous Mind\n",
      "3312                                 Apollo 18\n",
      "26                   Captain America Civil War\n",
      "Name: movie_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "exec(msg_311.replace('path_to_your_dataset.csv', \"final_merged_df.csv\").replace(\"print(movies_dataset.head())\",\"\").replace(\"example_movie_title\",\"Avatar\").replace(replace1,replacement1))"
   ]
  }
 ],
 "metadata": {
  "ai8-sym": {
   "notebook_id": "8c60fb97-965b-4247-99ff-aca9b896ec91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
